{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTMModel3L(nn.Module):\n",
    "    def __init__(self, in_dim=11, hidden_size=300, num_layers=1, output_size=2):\n",
    "        super().__init__()\n",
    "        self.lstm_1 = nn.LSTM(in_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_2 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_3 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm_1(x)\n",
    "        x, _ = self.lstm_2(x)\n",
    "        x, _ = self.lstm_3(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# Instantiate and load weights\n",
    "model = LSTMModel3L()\n",
    "ckpt_path = \"best_model.pt\"  # <-- change to your checkpoint\n",
    "torch.save(model, ckpt_path)\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# Some checkpoints save model.state_dict() inside a dict\n",
    "if isinstance(state, dict) and \"state_dict\" in state:\n",
    "    state = state[\"state_dict\"]\n",
    "elif hasattr(state, \"state_dict\"):\n",
    "    state = state.state_dict()\n",
    "\n",
    "# remove unwanted prefixes (e.g. \"module.\" if saved from DDP)\n",
    "state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping model weight and saving it in npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 14 tensors → state_dict_npz.npz\n",
      "Sample keys: ['lstm_1.weight_ih_l0', 'lstm_1.weight_hh_l0', 'lstm_1.bias_ih_l0', 'lstm_1.bias_hh_l0', 'lstm_2.weight_ih_l0', 'lstm_2.weight_hh_l0', 'lstm_2.bias_ih_l0', 'lstm_2.bias_hh_l0', 'lstm_3.weight_ih_l0', 'lstm_3.weight_hh_l0', 'lstm_3.bias_ih_l0', 'lstm_3.bias_hh_l0', 'fc.weight', 'fc.bias']\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "\n",
    "ckpt_path = \"best_model.pt\"   # <-- change me\n",
    "\n",
    "obj = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# 1) Pick the right sub-dict containing tensors\n",
    "if isinstance(obj, dict):\n",
    "    if \"model_state_dict\" in obj:\n",
    "        sd = obj[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in obj:\n",
    "        sd = obj[\"state_dict\"]\n",
    "    elif any(torch.is_tensor(v) for v in obj.values()):\n",
    "        sd = obj\n",
    "    else:\n",
    "        raise ValueError(\"Could not find model weights in checkpoint dict.\")\n",
    "elif hasattr(obj, \"state_dict\"):\n",
    "    sd = obj.state_dict()\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized checkpoint format\")\n",
    "\n",
    "# 2) Flatten one level if some entries are nested dicts\n",
    "flat = {}\n",
    "for k, v in sd.items():\n",
    "    if isinstance(v, dict):\n",
    "        for kk, vv in v.items():\n",
    "            if torch.is_tensor(vv):\n",
    "                flat[f\"{k}.{kk}\"] = vv\n",
    "    elif torch.is_tensor(v):\n",
    "        flat[k] = v\n",
    "\n",
    "# 3) Strip common wrappers (DDP/Lightning/etc.)\n",
    "def strip_prefix(k):\n",
    "    for p in (\"module.\", \"model.\", \"net.\", \"student.\"):\n",
    "        if k.startswith(p):\n",
    "            return k[len(p):]\n",
    "    return k\n",
    "\n",
    "flat = {strip_prefix(k): v for k, v in flat.items()}\n",
    "\n",
    "# 4) Save to NPZ\n",
    "npz_path = \"state_dict_npz.npz\"\n",
    "np.savez(npz_path, **{k: v.detach().cpu().numpy() for k, v in flat.items()})\n",
    "print(f\"✅ Saved {len(flat)} tensors → {npz_path}\")\n",
    "print(\"Sample keys:\", list(flat.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking dict keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lstm_1.weight_ih_l0', 'lstm_1.weight_hh_l0', 'lstm_1.bias_ih_l0', 'lstm_1.bias_hh_l0', 'lstm_2.weight_ih_l0', 'lstm_2.weight_hh_l0', 'lstm_2.bias_ih_l0', 'lstm_2.bias_hh_l0', 'lstm_3.weight_ih_l0', 'lstm_3.weight_hh_l0', 'lstm_3.bias_ih_l0', 'lstm_3.bias_hh_l0', 'fc.weight', 'fc.bias']\n"
     ]
    }
   ],
   "source": [
    "print(list(flat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tf model and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 16:12:54.555529: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights copied ✅\n",
      "Forward OK, output shape: (1, 2)\n",
      "INFO:tensorflow:Assets written to: tf_export_lstm3_unrolled_static/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_export_lstm3_unrolled_static/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel → tf_export_lstm3_unrolled_static\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1763548979.973434  322125 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1763548979.973459  322125 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-19 16:12:59.973737: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: tf_export_lstm3_unrolled_static\n",
      "2025-11-19 16:12:59.980963: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-11-19 16:12:59.980979: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: tf_export_lstm3_unrolled_static\n",
      "I0000 00:00:1763548980.050670  322125 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n",
      "2025-11-19 16:13:00.060177: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-11-19 16:13:00.184634: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: tf_export_lstm3_unrolled_static\n",
      "2025-11-19 16:13:00.295498: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 321764 microseconds.\n",
      "2025-11-19 16:13:00.455840: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-19 16:13:01.171660: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:4150] Estimated count of arithmetic ops: 31.981 M  ops, equivalently 15.990 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite FP32 → tf_export_lstm3_unrolled_static/model_fp32.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1763548985.183335  322125 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1763548985.183358  322125 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-19 16:13:05.183497: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: tf_export_lstm3_unrolled_static\n",
      "2025-11-19 16:13:05.191144: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-11-19 16:13:05.191159: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: tf_export_lstm3_unrolled_static\n",
      "2025-11-19 16:13:05.261787: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-11-19 16:13:05.384534: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: tf_export_lstm3_unrolled_static\n",
      "2025-11-19 16:13:05.494318: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 310824 microseconds.\n",
      "2025-11-19 16:13:06.406946: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:4150] Estimated count of arithmetic ops: 31.981 M  ops, equivalently 15.990 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite FP16 → tf_export_lstm3_unrolled_static/model_fp16.tflite\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ---- config (static) ----\n",
    "BATCH, SEQ = 1, 100\n",
    "HID, OUT   = 300, 2\n",
    "NPZ        = \"state_dict_npz.npz\"\n",
    "SAVE_DIR   = \"tf_export_lstm3_unrolled_static\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # optional for stable numerics\n",
    "\n",
    "sd = np.load(NPZ)\n",
    "\n",
    "# infer IN_DIM from lstm_1 weights; force 4 layers\n",
    "Wi1 = sd[\"lstm_1.weight_ih_l0\"]           # (4H, IN_DIM)\n",
    "IN_DIM = Wi1.shape[1]\n",
    "assert Wi1.shape[0] // 4 == HID, \"HID mismatch with NPZ weights\"\n",
    "N_LAYERS = 3\n",
    "\n",
    "# ---- build 4×LSTM (unrolled) ----\n",
    "inp = keras.Input(shape=(SEQ, IN_DIM), name=\"input\")\n",
    "x = inp\n",
    "for i in range(1, N_LAYERS + 1):\n",
    "    x = layers.LSTM(\n",
    "        HID, return_sequences=True, unroll=True,\n",
    "        unit_forget_bias=False, activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\", name=f\"lstm_{i}\"\n",
    "    )(x)\n",
    "x = layers.Lambda(lambda t: t[:, -1, :], name=\"take_last_t\")(x)\n",
    "out = layers.Dense(OUT, name=\"fc\")(x)\n",
    "model = keras.Model(inp, out, name=\"LSTMModel3L_unrolled\")\n",
    "\n",
    "# ---- copy weights ----\n",
    "def copy_lstm(i):\n",
    "    Wi = sd[f\"lstm_{i}.weight_ih_l0\"]   # (4H, in)\n",
    "    Wh = sd[f\"lstm_{i}.weight_hh_l0\"]   # (4H, H)\n",
    "    bi = sd[f\"lstm_{i}.bias_ih_l0\"]     # (4H,)\n",
    "    bh = sd[f\"lstm_{i}.bias_hh_l0\"]     # (4H,)\n",
    "    model.get_layer(f\"lstm_{i}\").set_weights([Wi.T, Wh.T, bi + bh])\n",
    "\n",
    "for i in range(1, N_LAYERS + 1):\n",
    "    copy_lstm(i)\n",
    "\n",
    "W = sd[\"fc.weight\"]   # (OUT, HID)\n",
    "b = sd[\"fc.bias\"]     # (OUT,)\n",
    "model.get_layer(\"fc\").set_weights([W.T, b])\n",
    "print(\"Weights copied ✅\")\n",
    "\n",
    "# quick run\n",
    "y = model(np.random.randn(BATCH, SEQ, IN_DIM).astype(np.float32))\n",
    "print(\"Forward OK, output shape:\", y.shape)\n",
    "\n",
    "# ---- export SavedModel (static signature) ----\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "@tf.function(input_signature=[tf.TensorSpec([BATCH, SEQ, IN_DIM], tf.float32, name=\"input\")])\n",
    "def serve(x): return {\"output\": model(x, training=False)}\n",
    "tf.saved_model.save(model, SAVE_DIR, signatures={\"serving_default\": serve.get_concrete_function()})\n",
    "print(\"SavedModel →\", SAVE_DIR)\n",
    "\n",
    "# ---- TFLite FP32 ----\n",
    "conv = tf.lite.TFLiteConverter.from_saved_model(SAVE_DIR)\n",
    "open(os.path.join(SAVE_DIR, \"model_fp32.tflite\"), \"wb\").write(conv.convert())\n",
    "print(\"TFLite FP32 →\", os.path.join(SAVE_DIR, \"model_fp32.tflite\"))\n",
    "\n",
    "# ---- TFLite FP16 ----\n",
    "conv = tf.lite.TFLiteConverter.from_saved_model(SAVE_DIR)\n",
    "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "conv.target_spec.supported_types = [tf.float16]\n",
    "open(os.path.join(SAVE_DIR, \"model_fp16.tflite\"), \"wb\").write(conv.convert())\n",
    "print(\"TFLite FP16 →\", os.path.join(SAVE_DIR, \"model_fp16.tflite\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample i_o saving of torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote parity_io_pt.npz\n"
     ]
    }
   ],
   "source": [
    "# ---- PyTorch parity export ----\n",
    "import numpy as np, torch, torch.nn as nn\n",
    "\n",
    "ckpt_path = \"best_model.pt\"  # <-- change me\n",
    "SEQ, HID, OUT = 100, 300, 2\n",
    "\n",
    "class LSTMModelV3(nn.Module):\n",
    "    def __init__(self, in_dim=11, hidden_size=HID, num_layers=1, output_size=OUT):\n",
    "        super().__init__()\n",
    "        self.lstm_1 = nn.LSTM(in_dim,       hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_2 = nn.LSTM(hidden_size,  hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_3 = nn.LSTM(hidden_size,  hidden_size, num_layers, batch_first=True)\n",
    "        # self.lstm_4 = nn.LSTM(hidden_size,  hidden_size, num_layers, batch_first=True)\n",
    "        self.fc     = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        o1,_ = self.lstm_1(x)\n",
    "        o2,_ = self.lstm_2(o1)\n",
    "        o3,_ = self.lstm_3(o2)\n",
    "        # o4,_ = self.lstm_4(o3)\n",
    "        y = self.fc(o3[:, -1, :])\n",
    "        return y, (o1,o2,o3)\n",
    "\n",
    "# load checkpoint (handles common formats)\n",
    "obj = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "if hasattr(obj, \"state_dict\"):\n",
    "    sd = obj.state_dict()\n",
    "elif isinstance(obj, dict) and \"model_state_dict\" in obj:\n",
    "    sd = obj[\"model_state_dict\"]\n",
    "elif isinstance(obj, dict) and \"state_dict\" in obj:\n",
    "    sd = obj[\"state_dict\"]\n",
    "else:\n",
    "    sd = obj\n",
    "sd = {k.replace(\"module.\", \"\"): v for k,v in sd.items()}\n",
    "\n",
    "in_dim = sd[\"lstm_1.weight_ih_l0\"].shape[1]\n",
    "m = LSTMModelV3(in_dim=in_dim).eval()\n",
    "m.load_state_dict(sd, strict=False)\n",
    "\n",
    "# fixed input\n",
    "rng = np.random.default_rng(123)\n",
    "x_np = rng.standard_normal((1, SEQ, in_dim), dtype=np.float32)\n",
    "with torch.no_grad():\n",
    "    y_pt, (o1,o2,o3) = m(torch.from_numpy(x_np))\n",
    "y_pt = y_pt.cpu().numpy()\n",
    "o1,o2,o3 = [t.cpu().numpy() for t in (o1,o2,o3)]\n",
    "\n",
    "np.savez(\"parity_io_pt.npz\",\n",
    "         x=x_np, y_pt=y_pt, o1=o1, o2=o2, o3=o3,\n",
    "         in_dim=in_dim, hid=HID, seq=SEQ, out_dim=OUT)\n",
    "print(\"wrote parity_io_pt.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Tf model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_DIM=11, HID=300, OUT=2, SEQ=100\n",
      "Weights set on rebuilt Keras model ✅\n",
      "Loaded PyTorch reference ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjav.singh/.local/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input']\n",
      "Received: inputs=Tensor(shape=(1, 100, 11))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF vs PyTorch (per-layer + FC) ---\n",
      "LSTM1    shape=(1, 100, 300)  max%=0.098%  mean%=0.000%  RMSE%=0.001%\n",
      "LSTM2    shape=(1, 100, 300)  max%=0.257%  mean%=0.000%  RMSE%=0.002%\n",
      "LSTM3    shape=(1, 100, 300)  max%=1.138%  mean%=0.000%  RMSE%=0.007%\n",
      "FC_OUT   shape=(1, 2)  max%=0.000%  mean%=0.000%  RMSE%=0.000%\n",
      "\n",
      "--- SavedModel vs rebuilt Keras (final FC) ---\n",
      "SM_OUT   shape=(1, 2)  max%=0.000%  mean%=0.000%  RMSE%=0.000%\n",
      "\n",
      "PARITY: PASS ✅\n"
     ]
    }
   ],
   "source": [
    "# ---- TF parity check that doesn't require keras load_model on SavedModel ----\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "MODEL_DIR = \"tf_export_lstm3_unrolled_static\"  # your exported SavedModel\n",
    "NPZ_WEIGHTS = \"state_dict_npz.npz\"             # Torch → NPZ weights\n",
    "PT_REF = \"parity_io_pt.npz\"                    # PyTorch reference (per-layer + final)\n",
    "\n",
    "# 1) Load NPZ weights and infer shapes\n",
    "sd = np.load(NPZ_WEIGHTS)\n",
    "Wi1 = sd[\"lstm_1.weight_ih_l0\"]           # (4H, IN_DIM)\n",
    "HID = Wi1.shape[0] // 4\n",
    "IN_DIM = Wi1.shape[1]\n",
    "SEQ = 100     # use the same you trained/exported\n",
    "OUT = sd[\"fc.weight\"].shape[0]\n",
    "print(f\"IN_DIM={IN_DIM}, HID={HID}, OUT={OUT}, SEQ={SEQ}\")\n",
    "\n",
    "# 2) Rebuild the unrolled 4×LSTM Keras model (to expose intermediate layers)\n",
    "inp = keras.Input(shape=(SEQ, IN_DIM), name=\"input\")\n",
    "x = inp\n",
    "for i in range(1, 4):\n",
    "    x = layers.LSTM(\n",
    "        HID, return_sequences=True, unroll=True,\n",
    "        unit_forget_bias=False, activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\", name=f\"lstm_{i}\"\n",
    "    )(x)\n",
    "x = layers.Lambda(lambda t: t[:, -1, :], name=\"take_last_t\")(x)\n",
    "out = layers.Dense(OUT, name=\"fc\")(x)\n",
    "model = keras.Model(inp, out, name=\"LSTMModel3L_unrolled\")\n",
    "\n",
    "# 3) Copy weights from NPZ → Keras\n",
    "def copy_lstm(i):\n",
    "    Wi = sd[f\"lstm_{i}.weight_ih_l0\"]   # (4H, in)\n",
    "    Wh = sd[f\"lstm_{i}.weight_hh_l0\"]   # (4H, H)\n",
    "    bi = sd[f\"lstm_{i}.bias_ih_l0\"]     # (4H,)\n",
    "    bh = sd[f\"lstm_{i}.bias_hh_l0\"]     # (4H,)\n",
    "    model.get_layer(f\"lstm_{i}\").set_weights([Wi.T, Wh.T, bi + bh])\n",
    "\n",
    "for i in range(1, 4):\n",
    "    copy_lstm(i)\n",
    "\n",
    "W = sd[\"fc.weight\"]  # (OUT, HID)\n",
    "b = sd[\"fc.bias\"]    # (OUT,)\n",
    "model.get_layer(\"fc\").set_weights([W.T, b])\n",
    "\n",
    "print(\"Weights set on rebuilt Keras model ✅\")\n",
    "\n",
    "# 4) Load PyTorch reference I/O\n",
    "ref = np.load(PT_REF)\n",
    "x_np  = ref[\"x\"]\n",
    "y_pt  = ref[\"y_pt\"]\n",
    "o1_pt = ref[\"o1\"]; o2_pt = ref[\"o2\"]; o3_pt = ref[\"o3\"]\n",
    "print(\"Loaded PyTorch reference ✅\")\n",
    "\n",
    "# 5) Get intermediate outputs from rebuilt Keras model\n",
    "l1 = model.get_layer(\"lstm_1\").output\n",
    "l2 = model.get_layer(\"lstm_2\").output\n",
    "l3 = model.get_layer(\"lstm_3\").output\n",
    "# l4 = model.get_layer(\"lstm_4\").output\n",
    "mid = keras.Model(model.inputs, [l1, l2, l3, model.output])\n",
    "\n",
    "o1_tf, o2_tf, o3_tf,  y_tf = mid(x_np, training=False)\n",
    "o1_tf, o2_tf, o3_tf,  y_tf = [np.asarray(t, dtype=np.float32) for t in (o1_tf,o2_tf,o3_tf,y_tf)]\n",
    "\n",
    "# 6) Also verify the exported SavedModel final output equals the rebuilt Keras output\n",
    "loaded = tf.saved_model.load(MODEL_DIR)\n",
    "infer = loaded.signatures[\"serving_default\"]\n",
    "y_saved = infer(tf.constant(x_np))[\"output\"].numpy().astype(np.float32)\n",
    "\n",
    "def report(name, a, b):\n",
    "    diff = np.abs(a - b)\n",
    "    denom = np.maximum(np.abs(b), 1e-8)  # avoid div-by-zero\n",
    "    pct_err = (diff / denom) * 100\n",
    "    print(f\"{name:7s}  shape={a.shape}  \"\n",
    "          f\"max%={pct_err.max():.3f}%  mean%={pct_err.mean():.3f}%  \"\n",
    "          f\"RMSE%={np.sqrt((pct_err**2).mean()):.3f}%\")\n",
    "\n",
    "\n",
    "print(\"\\n--- TF vs PyTorch (per-layer + FC) ---\")\n",
    "report(\"LSTM1\", o1_tf, o1_pt)\n",
    "report(\"LSTM2\", o2_tf, o2_pt)\n",
    "report(\"LSTM3\", o3_tf, o3_pt)\n",
    "# report(\"LSTM4\", o4_tf, o4_pt)\n",
    "report(\"FC_OUT\", y_tf,  y_pt)\n",
    "\n",
    "print(\"\\n--- SavedModel vs rebuilt Keras (final FC) ---\")\n",
    "report(\"SM_OUT\", y_saved, y_tf)\n",
    "\n",
    "tol = 5e-6\n",
    "ok_layers = all(np.max(np.abs(a - b)) < tol for a,b in\n",
    "                [(o1_tf,o1_pt), (o2_tf,o2_pt), (o3_tf,o3_pt), (y_tf,y_pt)])\n",
    "ok_export = np.max(np.abs(y_saved - y_tf)) < tol\n",
    "print(\"\\nPARITY:\", \"PASS ✅\" if (ok_layers and ok_export) else \"MISMATCH ❌\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with tflite fp32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TFLite FP32 vs TF (SavedModel) ---\n",
      "TFLite→TF  shape=(1, 2)  max%=nan%  mean%=nan%  RMSE%=nan%\n",
      "\n",
      "--- TFLite FP32 vs PyTorch ---\n",
      "TFLite→PT  shape=(1, 2)  max%=nan%  mean%=nan%  RMSE%=nan%\n",
      "\n",
      "PARITY (TFLite FP32): MISMATCH ❌\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------- Paths ----------\n",
    "MODEL_DIR   = \"tf_export_lstm3_unrolled_static\"\n",
    "TFLITE_FP32 = os.path.join(MODEL_DIR, \"model_fp32.tflite\")\n",
    "PT_REF      = \"parity_io_pt.npz\"\n",
    "\n",
    "# ---------- Load reference input & PyTorch output ----------\n",
    "io = np.load(PT_REF)\n",
    "x_np = io[\"x\"]                  # shape (1, 100, IN_DIM)\n",
    "y_pt = io[\"y_pt\"].astype(np.float32)\n",
    "\n",
    "# ---------- Get TensorFlow (SavedModel) output to use as TF reference ----------\n",
    "saved = tf.saved_model.load(MODEL_DIR)\n",
    "infer = saved.signatures[\"serving_default\"]\n",
    "y_tf = infer(tf.constant(x_np))[\"output\"].numpy().astype(np.float32)\n",
    "\n",
    "# ---------- Run TFLite FP32 ----------\n",
    "# Prefer tflite_runtime if installed; otherwise fallback to tf.lite.Interpreter\n",
    "try:\n",
    "    import tflite_runtime.interpreter as tflite\n",
    "    Interpreter = tflite.Interpreter\n",
    "except Exception:\n",
    "    Interpreter = tf.lite.Interpreter\n",
    "\n",
    "# Interpreter = tf.lite.Interpreter\n",
    "\n",
    "interp = Interpreter(model_path=TFLITE_FP32, num_threads=1)\n",
    "interp.allocate_tensors()\n",
    "\n",
    "in_details  = interp.get_input_details()\n",
    "out_details = interp.get_output_details()\n",
    "\n",
    "# If the TFLite input isn't the shape we need, resize it\n",
    "need_shape = x_np.shape\n",
    "if tuple(in_details[0][\"shape\"]) != need_shape:\n",
    "    interp.resize_tensor_input(in_details[0][\"index\"], need_shape, strict=False)\n",
    "    interp.allocate_tensors()\n",
    "    in_details  = interp.get_input_details()\n",
    "    out_details = interp.get_output_details()\n",
    "\n",
    "# Ensure dtype is float32\n",
    "x_feed = x_np.astype(np.float32)\n",
    "\n",
    "interp.set_tensor(in_details[0][\"index\"], x_feed)\n",
    "interp.invoke()\n",
    "y_tfl = interp.get_tensor(out_details[0][\"index\"]).astype(np.float32)\n",
    "\n",
    "# ---------- % error reporting ----------\n",
    "def report_pct(name, pred, ref):\n",
    "    diff = np.abs(pred - ref)\n",
    "    denom = np.maximum(np.abs(ref), 1e-8)    # avoid div-by-zero\n",
    "    pct = (diff / denom) * 100.0\n",
    "    print(f\"{name:10s} shape={pred.shape}  \"\n",
    "          f\"max%={pct.max():.6f}%  mean%={pct.mean():.6f}%  RMSE%={np.sqrt((pct**2).mean()):.6f}%\")\n",
    "\n",
    "print(\"\\n--- TFLite FP32 vs TF (SavedModel) ---\")\n",
    "report_pct(\"TFLite→TF\", y_tfl, y_tf)\n",
    "\n",
    "print(\"\\n--- TFLite FP32 vs PyTorch ---\")\n",
    "report_pct(\"TFLite→PT\", y_tfl, y_pt)\n",
    "\n",
    "# Optional gate (tight for FP32): pass if both comparisons are extremely close\n",
    "tol_pct_max = 0.001  # 0.001% max abs % error\n",
    "ok_tf = (np.max(np.abs((y_tfl - y_tf) / np.maximum(np.abs(y_tf), 1e-8))) * 100) < tol_pct_max\n",
    "ok_pt = (np.max(np.abs((y_tfl - y_pt) / np.maximum(np.abs(y_pt), 1e-8))) * 100) < tol_pct_max\n",
    "print(\"\\nPARITY (TFLite FP32):\", \"PASS ✅\" if (ok_tf and ok_pt) else \"MISMATCH ❌\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 100 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote parity_pack_pt.npz with 100 samples\n"
     ]
    }
   ],
   "source": [
    "# make_parity_pack.py  (run in a PyTorch env)\n",
    "import numpy as np, torch, torch.nn as nn\n",
    "\n",
    "ckpt_path = \"best_model.pt\"   # <-- set this\n",
    "# make_parity_pack.py  (run in a PyTorch env)\n",
    "\n",
    "SEQ = 100\n",
    "HID = 400\n",
    "OUT = 2\n",
    "N   = 100\n",
    "SEED = 123\n",
    "\n",
    "class LSTMModelV3(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_size=HID, num_layers=1, output_size=OUT):\n",
    "        super().__init__()\n",
    "        self.lstm_1 = nn.LSTM(in_dim,      hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_2 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_3 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.lstm_4 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc     = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm_1(x); x,_ = self.lstm_2(x)\n",
    "        x,_ = self.lstm_3(x)\n",
    "        # x,_ = self.lstm_4(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# load checkpoint\n",
    "obj = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "if hasattr(obj, \"state_dict\"):\n",
    "    sd = obj.state_dict()\n",
    "elif isinstance(obj, dict) and \"model_state_dict\" in obj:\n",
    "    sd = obj[\"model_state_dict\"]\n",
    "elif isinstance(obj, dict) and \"state_dict\" in obj:\n",
    "    sd = obj[\"state_dict\"]\n",
    "else:\n",
    "    sd = obj\n",
    "sd = {k.replace(\"module.\", \"\"): v for k,v in sd.items()}\n",
    "\n",
    "in_dim = sd[\"lstm_1.weight_ih_l0\"].shape[1]\n",
    "m = LSTMModelV3(in_dim).eval()\n",
    "m.load_state_dict(sd, strict=False)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "X = rng.standard_normal((N, SEQ, in_dim), dtype=np.float32)  # no batch dim here\n",
    "Y_pt = np.zeros((N, OUT), dtype=np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        xi = torch.from_numpy(X[i:i+1])          # (1,SEQ,IN_DIM) keep batch=1\n",
    "        yi = m(xi).cpu().numpy()                 # (1,OUT)\n",
    "        Y_pt[i] = yi[0]\n",
    "\n",
    "np.savez(\"parity_pack_pt.npz\",\n",
    "         x=X, y_pt=Y_pt, seq=SEQ, in_dim=in_dim, out_dim=OUT, n=N)\n",
    "print(\"wrote parity_pack_pt.npz with\", N, \"samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing N=100 samples | X=(100, 100, 11), Y_pt=(100, 2), Y_tf=(100, 2), Y_tfl=(100, 2)\n",
      "\n",
      "TFLite FP32 vs TF:\n",
      "  overall   -> max%=nan%  mean%=nan%  RMSE%=nan%\n",
      "  per-sample-> max%: mean=nan%  p95=nan%  max=nan% @ idx=0\n",
      "TFLite FP32 vs PT:\n",
      "  overall   -> max%=nan%  mean%=nan%  RMSE%=nan%\n",
      "  per-sample-> max%: mean=nan%  p95=nan%  max=nan% @ idx=0\n",
      "TF vs PT:\n",
      "  overall   -> max%=0.002269%  mean%=0.000058%  RMSE%=0.000187%\n",
      "  per-sample-> max%: mean=0.000094%  p95=0.000217%  max=0.002269% @ idx=79\n",
      "\n",
      "PASS (TF vs PT): ✅\n",
      "PASS (TFLite FP32 vs TF & PT): ❌\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, tensorflow as tf\n",
    "\n",
    "MODEL_DIR   = \"tf_export_lstm3_unrolled_static\"\n",
    "TFLITE_FP32 = os.path.join(MODEL_DIR, \"model_fp32.tflite\")\n",
    "PACK        = \"parity_pack_pt.npz\"\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# ---- load pack ----\n",
    "pack = np.load(PACK)\n",
    "X    = pack[\"x\"]      # (N, SEQ, IN_DIM)\n",
    "Y_pt = pack[\"y_pt\"]   # (N, OUT)\n",
    "N, SEQ, IN_DIM = X.shape\n",
    "OUT = Y_pt.shape[-1]\n",
    "\n",
    "# ---- TF SavedModel infer ----\n",
    "saved = tf.saved_model.load(MODEL_DIR)\n",
    "infer = saved.signatures[\"serving_default\"]\n",
    "\n",
    "Y_tf = np.zeros((N, OUT), dtype=np.float32)\n",
    "for i in range(N):\n",
    "    xi = X[i:i+1]                                # (1,SEQ,IN_DIM)\n",
    "    yi = infer(tf.constant(xi))[\"output\"].numpy().astype(np.float32)  # (1,OUT)\n",
    "    Y_tf[i] = yi[0]\n",
    "\n",
    "# ---- TFLite FP32 infer ----\n",
    "try:\n",
    "    import tflite_runtime.interpreter as tflite\n",
    "    Interpreter = tflite.Interpreter\n",
    "except Exception:\n",
    "    Interpreter = tf.lite.Interpreter\n",
    "\n",
    "# Interpreter = tf.lite.Interpreter\n",
    "\n",
    "interp = Interpreter(model_path=TFLITE_FP32, num_threads=1)\n",
    "interp.allocate_tensors()\n",
    "in_det  = interp.get_input_details()[0]\n",
    "out_det = interp.get_output_details()[0]\n",
    "\n",
    "def run_tfl(xi):  # xi: (1,SEQ,IN_DIM)\n",
    "    if tuple(in_det[\"shape\"]) != xi.shape:\n",
    "        interp.resize_tensor_input(in_det[\"index\"], xi.shape, strict=False)\n",
    "        interp.allocate_tensors()\n",
    "    interp.set_tensor(in_det[\"index\"], xi.astype(np.float32))\n",
    "    interp.invoke()\n",
    "    return interp.get_tensor(out_det[\"index\"]).astype(np.float32)  # (1,OUT)\n",
    "\n",
    "Y_tfl = np.zeros((N, OUT), dtype=np.float32)\n",
    "for i in range(N):\n",
    "    Y_tfl[i] = run_tfl(X[i:i+1])[0]\n",
    "\n",
    "# ---- % error helpers ----\n",
    "def pct_err(pred, ref):\n",
    "    denom = np.maximum(np.abs(ref), 1e-8)\n",
    "    return np.abs(pred - ref) / denom * 100.0\n",
    "\n",
    "def summarize_pct(name, pred, ref):\n",
    "    pe = pct_err(pred, ref)           # (N, OUT)\n",
    "    max_each  = pe.max(axis=1)        # per-sample max%\n",
    "    mean_each = pe.mean(axis=1)       # per-sample mean%\n",
    "    rmse_each = np.sqrt((pe**2).mean(axis=1))\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  overall   -> max%={pe.max():.6f}%  mean%={pe.mean():.6f}%  RMSE%={np.sqrt((pe**2).mean()):.6f}%\")\n",
    "    worst = int(np.argmax(max_each))\n",
    "    print(f\"  per-sample-> max%: mean={max_each.mean():.6f}%  p95={np.percentile(max_each,95):.6f}%  max={max_each.max():.6f}% @ idx={worst}\")\n",
    "    return max_each, mean_each, rmse_each\n",
    "\n",
    "print(f\"Comparing N={N} samples | X={X.shape}, Y_pt={Y_pt.shape}, Y_tf={Y_tf.shape}, Y_tfl={Y_tfl.shape}\\n\")\n",
    "\n",
    "m_tfl_tf, _, _ = summarize_pct(\"TFLite FP32 vs TF\",  Y_tfl, Y_tf)\n",
    "m_tfl_pt, _, _ = summarize_pct(\"TFLite FP32 vs PT\",  Y_tfl, Y_pt)\n",
    "m_tf_pt,  _, _ = summarize_pct(\"TF vs PT\",           Y_tf,  Y_pt)\n",
    "\n",
    "# ---- pass gates ----\n",
    "tol_max_pct = 0.01  # 0.001% max abs % error\n",
    "pass_tf  = (m_tf_pt.max()   < tol_max_pct)     # TF vs PT\n",
    "pass_tfl = (m_tfl_tf.max()  < tol_max_pct) and (m_tfl_pt.max() < tol_max_pct)  # TFLite vs both\n",
    "print(\"\\nPASS (TF vs PT):\",  \"✅\" if pass_tf  else \"❌\")\n",
    "print(\"PASS (TFLite FP32 vs TF & PT):\", \"✅\" if pass_tfl else \"❌\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2203637   1.2037135 ]\n",
      " [ 1.1469762   0.6399808 ]\n",
      " [ 1.7424899   0.52624094]\n",
      " [ 0.3726699   0.8699015 ]\n",
      " [-1.0564709   1.9162472 ]\n",
      " [ 1.2260323   0.27355912]\n",
      " [-0.5207463   1.3430268 ]\n",
      " [-0.01862868  0.28096434]\n",
      " [-0.8239329   0.8985657 ]\n",
      " [-0.49413708  1.8043662 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pt[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22036368  1.2037143 ]\n",
      " [ 1.1469762   0.63998055]\n",
      " [ 1.7424885   0.5262406 ]\n",
      " [ 0.3726697   0.869901  ]\n",
      " [-1.0564705   1.916247  ]\n",
      " [ 1.2260323   0.27355888]\n",
      " [-0.52074635  1.3430262 ]\n",
      " [-0.01862887  0.28096434]\n",
      " [-0.8239332   0.8985659 ]\n",
      " [-0.49413693  1.8043659 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_tf[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_tfl[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
