{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c69ee5",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139151f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Minimal Transformer Encoder Layer WITHOUT LayerNorm\n",
    "# ------------------------------------------------------------\n",
    "class SimpleTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model=1000, nhead=10, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # No dropout, no LayerNorm\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        # ---- Self Attention ----\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + attn_out  # residual\n",
    "\n",
    "        # ---- Feed-forward ----\n",
    "        ff = self.linear2(self.act(self.linear1(x)))\n",
    "        x = x + ff  # residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Decoder-Only Transformer (GPT-style)\n",
    "# ------------------------------------------------------------\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=11,\n",
    "        model_dim=1000,\n",
    "        num_layers=3,\n",
    "        dim_feedforward=2048,\n",
    "        nhead=10,\n",
    "        out_dim=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # project sensor input → transformer dimension\n",
    "        self.input_proj = nn.Linear(in_dim, model_dim)\n",
    "\n",
    "        # build N layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                SimpleTransformerLayer(\n",
    "                    d_model=model_dim, nhead=nhead, dim_feedforward=dim_feedforward\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # final output head (predict 2 values)\n",
    "        self.fc = nn.Linear(model_dim, out_dim)\n",
    "\n",
    "    def _causal_mask(self, T, device):\n",
    "        \"\"\"\n",
    "        Returns a [T, T] causal mask with -inf above diagonal.\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(T, T, device=device) * float(\"-inf\"), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, in_dim]\n",
    "        returns last-step prediction: [B, out_dim]\n",
    "        \"\"\"\n",
    "        B, T, _ = x.size()\n",
    "\n",
    "        # input embedding\n",
    "        x = self.input_proj(x)  # → [B, T, model_dim]\n",
    "\n",
    "        # causal mask\n",
    "        mask = self._causal_mask(T, x.device)\n",
    "\n",
    "        # transformer stack\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)  # [B, T, model_dim]\n",
    "\n",
    "        # Get only last timestep (like LSTM last hidden state)\n",
    "        last = x[:, -1, :]  # [B, model_dim]\n",
    "\n",
    "        # Final prediction\n",
    "        return self.fc(last)  # [B, out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc27cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and load weights\n",
    "model = SimpleTransformerLayer()\n",
    "ckpt_path = \"best_t_model.pt\"  # <-- change to your checkpoint\n",
    "torch.save(model, \"best_t_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85d8c221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 8 tensors → state_dict_npz.npz\n",
      "Sample keys: ['self_attn.in_proj_weight', 'self_attn.in_proj_bias', 'self_attn.out_proj.weight', 'self_attn.out_proj.bias', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "ckpt_path = \"best_t_model.pt\"   # <-- change me\n",
    "\n",
    "obj = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# 1) Pick the right sub-dict containing tensors\n",
    "if isinstance(obj, dict):\n",
    "    if \"model_state_dict\" in obj:\n",
    "        sd = obj[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in obj:\n",
    "        sd = obj[\"state_dict\"]\n",
    "    elif any(torch.is_tensor(v) for v in obj.values()):\n",
    "        sd = obj\n",
    "    else:\n",
    "        raise ValueError(\"Could not find model weights in checkpoint dict.\")\n",
    "elif hasattr(obj, \"state_dict\"):\n",
    "    sd = obj.state_dict()\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized checkpoint format\")\n",
    "\n",
    "# 2) Flatten one level if some entries are nested dicts\n",
    "flat = {}\n",
    "for k, v in sd.items():\n",
    "    if isinstance(v, dict):\n",
    "        for kk, vv in v.items():\n",
    "            if torch.is_tensor(vv):\n",
    "                flat[f\"{k}.{kk}\"] = vv\n",
    "    elif torch.is_tensor(v):\n",
    "        flat[k] = v\n",
    "\n",
    "# 3) Strip common wrappers (DDP/Lightning/etc.)\n",
    "def strip_prefix(k):\n",
    "    for p in (\"module.\", \"model.\", \"net.\", \"student.\"):\n",
    "        if k.startswith(p):\n",
    "            return k[len(p):]\n",
    "    return k\n",
    "\n",
    "flat = {strip_prefix(k): v for k, v in flat.items()}\n",
    "\n",
    "# 4) Save to NPZ\n",
    "npz_path = \"state_dict_npz.npz\"\n",
    "np.savez(npz_path, **{k: v.detach().cpu().numpy() for k, v in flat.items()})\n",
    "print(f\"✅ Saved {len(flat)} tensors → {npz_path}\")\n",
    "print(\"Sample keys:\", list(flat.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ---- config (static) ----\n",
    "BATCH, SEQ = 1, 100\n",
    "MODEL_DIM = 1000\n",
    "IN, OUT   = 11, 2\n",
    "NPZ        = \"state_dict_npz.npz\"\n",
    "SAVE_DIR   = \"tf_export_lstm3_unrolled_static\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # optional for stable numerics\n",
    "\n",
    "sd = np.load(NPZ)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
